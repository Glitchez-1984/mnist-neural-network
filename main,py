import numpy as np


def relu(x):
    return np.maximum(0, x)


def relu_derivative(x):
    return np.where(x > 0, 1, 0)


def softmax(x):
    exps = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exps / np.sum(exps, axis=1, keepdims=True)


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.weights1 = np.random.randn(self.input_size, self.hidden_size) / np.sqrt(self.input_size)
        self.weights2 = np.random.randn(self.hidden_size, self.output_size) / np.sqrt(self.hidden_size)
        self.biases1 = np.zeros((1, self.hidden_size))
        self.biases2 = np.zeros((1, self.output_size))

    def forward(self, X):
        self.z1 = np.dot(X, self.weights1) + self.biases1
        self.a1 = relu(self.z1)
        self.z2 = np.dot(self.a1, self.weights2) + self.biases2
        self.a2 = softmax(self.z2)
        return self.a2

    def backward(self, X, y, output, learning_rate, l2_reg):
        batch_size = len(X)

        delta2 = output - y
        delta1 = np.dot(delta2, self.weights2.T) * relu_derivative(self.a1)

        grad_weights2 = np.dot(self.a1.T, delta2) / batch_size + l2_reg * self.weights2
        grad_biases2 = np.sum(delta2, axis=0, keepdims=True) / batch_size
        grad_weights1 = np.dot(X.T, delta1) / batch_size + l2_reg * self.weights1
        grad_biases1 = np.sum(delta1, axis=0) / batch_size

        self.weights2 -= learning_rate * grad_weights2
        self.biases2 -= learning_rate * grad_biases2
        self.weights1 -= learning_rate * grad_weights1
        self.biases1 -= learning_rate * grad_biases1

    def train(self, X, y, epochs, learning_rate, batch_size, l2_reg):
        num_samples = len(X)

        for epoch in range(epochs):
            indices = np.random.permutation(num_samples)
            X = X[indices]
            y = y[indices]

            for i in range(0, num_samples, batch_size):
                batch_X = X[i:i + batch_size]
                batch_y = y[i:i + batch_size]

                output = self.forward(batch_X)
                self.backward(batch_X, batch_y, output, learning_rate, l2_reg)

            output = self.forward(X)
            predictions = np.argmax(output, axis=1)
            true_labels = np.argmax(y, axis=1)
            accuracy = np.mean(predictions == true_labels)

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}/{epochs} - Accuracy: {accuracy}")

    def predict(self, X):
        output = self.forward(X)
        predictions = np.argmax(output, axis=1)
        return predictions


def load_data(filename):
    data = np.loadtxt(filename, delimiter=',')
    X = data[:, 1:] / 255.0
    y = data[:, 0].astype(int)
    num_classes = np.max(y) + 1
    y = np.eye(num_classes)[y]
    return X, y


train_X, train_y = load_data('mnist_train.csv')
test_X, test_y = load_data('mnist_test.csv')

input_size = train_X.shape[1]
hidden_size = 256
output_size = train_y.shape[1]

nn = NeuralNetwork(input_size, hidden_size, output_size)
nn.train(train_X, train_y, epochs=100, learning_rate=0.01, batch_size=32, l2_reg=0.001)

predictions = nn.predict(test_X)
accuracy = np.mean(predictions == np.argmax(test_y, axis=1))

print(f"\nTest Accuracy: {accuracy}")
